{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hand_written_digit_reconizer.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOUrx9LFU2jnclSiK7Cmqh0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PFdXVGLVwwom","executionInfo":{"status":"ok","timestamp":1634474428494,"user_tz":-330,"elapsed":398,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}}},"source":["# mnist (benchmark) dataset\n","\n","# It is a dataset of handwritten images\n","\n","# http://yann.lecun.com/exdb/mnist/\n","\n","\n","# The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, \n","# and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been\n","# size-normalized and centered in a fixed-size image.\n","\n","# It is a good database for people who want to try learning techniques and pattern recognition methods on real-world \n","# data while spending minimal efforts on preprocessing and formatting.'''\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FPlZ8TLMPhs1","executionInfo":{"status":"ok","timestamp":1634474436707,"user_tz":-330,"elapsed":4045,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}},"outputId":"98ad02d8-b760-46c2-c69a-f179ccdb5f08"},"source":["!pip install keras"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n"]}]},{"cell_type":"code","metadata":{"id":"8DBVDzluPmEr","executionInfo":{"status":"ok","timestamp":1634474440243,"user_tz":-330,"elapsed":2104,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}}},"source":["import numpy as np \n","import tensorflow as tf\n","import keras  \n","from keras.datasets import mnist \n","from keras.models import Model \n","from keras.layers import Dense, Input\n","from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten \n","from keras import backend as k "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1gfKDcePqIk","executionInfo":{"status":"ok","timestamp":1634474442227,"user_tz":-330,"elapsed":394,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}},"outputId":"f74ebd4c-a117-40e8-cad9-40e3435a1cf3"},"source":["# If we load it, it is going to return tuple of NumPy Arrays\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()  "],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ruKzfR4uPzQ2","executionInfo":{"status":"ok","timestamp":1634474444218,"user_tz":-330,"elapsed":13,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}},"outputId":"660ec03c-15fd-490f-a640-63b379bdf258"},"source":["# Display the dataset\n","#we are storing 60k images with 28 by 28 pixels in grayscale format (60000, 28, 28)\n","# y_train contain NumPy array of digit labels (integers in range 0-9) with shape (60000,) \n","#we need to reshape it for further operations on it as Deep learning neural networks require that image data be provided as three-dimensional arrays\n","\n","\n","\n","print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))  \n","print('Test: X=%s, y=%s' % (x_test.shape, y_test.shape)) "],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Train: X=(60000, 28, 28), y=(60000,)\n","Test: X=(10000, 28, 28), y=(10000,)\n"]}]},{"cell_type":"code","metadata":{"id":"5GmfIZgwPJAb"},"source":["#There are two ways to represent the image data as a three dimensional array. The first involves having the channels as the last or third dimension in the \n","#array. This is called “channels last“. The second involves having the channels as the first dimension in the array, called “channels first“.\n","#TensorFlow: Channels last order.\n","#Theano: Channels first order.\n","#CNTK: Channels last order.\n","\n","#Channels Last. Image data is represented in a three-dimensional array where the last channel represents the color channels, e.g. [rows][cols][channels].\n","#Channels First. Image data is represented in a three-dimensional array where the first channel represents the color channels, e.g. [channels][rows][cols].\n","\n","#Since output of the model can comprise of any of the digits between 0 to 9.so, we need 10 classes in output. \n","#To make output for 10 classes, use keras.utils.to_categorical function, which will provide with the 10 columns. \n","#Out of these 10 columns only one value will be one and rest 9 will be zero and this one value of the output will denote the class of the digit."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hs0WK1YRQHr0","executionInfo":{"status":"ok","timestamp":1634474445821,"user_tz":-330,"elapsed":6,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}}},"source":["#Some image processing and deep learning libraries prefer channels first ordering, and some prefer channels last.\n","img_rows, img_cols=28, 28\n","  \n","if k.image_data_format() == 'channels_first': \n","  #reshape dataset to have a single channel\n","   x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)  # 1 represents grayscale and 3 will represent colored image\n","   x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) \n","   inpx = (1, img_rows, img_cols) \n","else:\n","  #reshape dataset to have a single channel\n","   x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)  # same here also\n","   x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) \n","   inpx = (img_rows, img_cols, 1)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Op7U_HPFQYxm","executionInfo":{"status":"ok","timestamp":1634474447255,"user_tz":-330,"elapsed":7,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}}},"source":["#convert from integers to floats\n","#the whole math for neural networks is continuous, not discrete, and this is best approximated with floating point numbers. The inputs, outputs, and \n","#weights of a neural network are continuous numbers.\n","#If we had integer outputs, they will still be converted to floating point at some point in the pipeline, in order to have compatible types where operations\n","#can be made. This might happen explicitly or implicitly, its better to be explicit about types.\n","\n","\n","x_train = x_train.astype('float32') \n","x_test = x_test.astype('float32') \n","\n","# Nor. 0 to 1\n","#We scale features to make them all of the same magnitude (i.e. importance or weight).\n","#You scale features to speed up execution time types = standard, minamx'''\n","\n","\n","x_train /= 255\n","x_test /= 255"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"lrveQU0IQlBC","executionInfo":{"status":"ok","timestamp":1634474449482,"user_tz":-330,"elapsed":705,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}}},"source":["# convert class vectors to binary class matrices  OR one hot encode target values\n","#from keras.utils import to_categorical\n","# first I need import tensorflow and then I can call to_categorical method!\n","\n","\n","import tensorflow as tf\n","from tensorflow.keras.utils import to_categorical\n","y_train = tf.keras.utils.to_categorical(y_train) \n","y_test = tf.keras.utils.to_categorical(y_test) \n","\n","\n","\n","#we need to convert our dataset into categorical format (and hence one-hot encoded format), we can do so using Scikit-learn’s OneHotEncoder module. \n","#However, TensorFlow also offers its own implementation: tensorflow.keras.utils.to_categorical. It’s a utility function which allows us to convert \n","#integer targets into categorical and hence one-hot encoded ones.\n","\n","#if y_train[index] = 1 then before applying to_categorical y_train[index].shape = () and after applying the method it will give y_train[index]\n","#and y_train[index].shape [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n","#(10,)  respectively"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ELR08nhQwhA","executionInfo":{"status":"ok","timestamp":1634474457654,"user_tz":-330,"elapsed":6271,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}}},"source":["# Creating three layers convolution and pooling\n","inpx = Input(shape=inpx) \n","#layer1 is Conv2d layer which convolves the image using 32 filters each of size (3*3).\n","layer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx) \n","\n","#layer2 is again a Conv2D layer which is also used to convolve the image and is using 64 filters each of size (3*3).\n","layer2 = Conv2D(64, (3, 3), activation='relu')(layer1) \n","\n","#layer3 is MaxPooling2D layer which picks the max value out of a matrix of size (3*3).\n","layer3 = MaxPooling2D(pool_size=(3, 3))(layer2) \n","\n","# To prevent overfitting layer4 is showing Dropout at a rate of 0.5\n","layer4 = Dropout(0.5)(layer3)\n","\n","#layer5 is flattening the output obtained from layer4 and this flatten output is passed to layer6.\n","layer5 = Flatten()(layer4) \n","\n","#layer6 is a hidden layer of neural network containng 250 neurons\n","layer6 = Dense(250, activation='sigmoid')(layer5) \n","\n","#layer7 is the output layer having 10 neurons for 10 classes of output that is using the softmax function\n","layer7 = Dense(10, activation='softmax')(layer6) "],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GxK8qnH7Q2s9","executionInfo":{"status":"ok","timestamp":1634475016975,"user_tz":-330,"elapsed":149440,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}},"outputId":"9a867147-3a08-4fcf-ff9f-f569be4cbe87"},"source":["# Here we are calling the compiler and fit function \n","# here we use Adadelta optimizer\n","\n","model = Model([inpx], layer7) \n","model.compile(optimizer=tf.keras.optimizers.Adadelta(), \n","              loss=tf.keras.losses.categorical_crossentropy, \n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train, epochs=20, batch_size=100) "],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","600/600 [==============================] - 8s 12ms/step - loss: 0.6283 - accuracy: 0.8310\n","Epoch 2/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.6180 - accuracy: 0.8334\n","Epoch 3/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.6086 - accuracy: 0.8357\n","Epoch 4/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5998 - accuracy: 0.8373\n","Epoch 5/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5926 - accuracy: 0.8378\n","Epoch 6/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5836 - accuracy: 0.8392\n","Epoch 7/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5773 - accuracy: 0.8410\n","Epoch 8/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5704 - accuracy: 0.8431\n","Epoch 9/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5616 - accuracy: 0.8458\n","Epoch 10/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5568 - accuracy: 0.8467\n","Epoch 11/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5499 - accuracy: 0.8502\n","Epoch 12/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5448 - accuracy: 0.8501\n","Epoch 13/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5388 - accuracy: 0.8504\n","Epoch 14/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5352 - accuracy: 0.8508\n","Epoch 15/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5292 - accuracy: 0.8533\n","Epoch 16/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5254 - accuracy: 0.8535\n","Epoch 17/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5206 - accuracy: 0.8547\n","Epoch 18/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5148 - accuracy: 0.8568\n","Epoch 19/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5110 - accuracy: 0.8578\n","Epoch 20/20\n","600/600 [==============================] - 7s 12ms/step - loss: 0.5088 - accuracy: 0.8568\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7faba00e9890>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"78hIdkaPQ8V5","executionInfo":{"status":"ok","timestamp":1634475022348,"user_tz":-330,"elapsed":1458,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}},"outputId":"1dbcc58d-5f1b-46e1-c9bb-0f22fa396f3a"},"source":["score = model.evaluate(x_test, y_test, verbose=1) #model.evaluate provide score for the data\n","print('loss=', score[0]) \n","print('accuracy=', score[1])\n"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.3937 - accuracy: 0.8907\n","loss= 0.3936682939529419\n","accuracy= 0.8906999826431274\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTWrk68OTY6b","executionInfo":{"status":"ok","timestamp":1634475045912,"user_tz":-330,"elapsed":1027,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}},"outputId":"5240bdb1-6624-4b5e-b7fb-8e38faa67615"},"source":["#sinle Value predication in number\n","predictions = model.predict(x_test)\n","print(np.argmax(np.round(predictions[3])))\n"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"eYh1XBzXTezE","executionInfo":{"status":"ok","timestamp":1634475057521,"user_tz":-330,"elapsed":415,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}},"outputId":"9d31e116-0ecb-4ffc-9561-ebb4e9068b11"},"source":["#sinle Value predication in graph (plot)\n","import matplotlib.pyplot as plt\n","plt.imshow(x_test[3].reshape(28, 28), cmap = plt.cm.binary)\n","plt.show()\n"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"aNcJX-qPTh2l","executionInfo":{"status":"ok","timestamp":1634475094906,"user_tz":-330,"elapsed":702,"user":{"displayName":"Himanshu Mall","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13334274629087873293"}}},"source":["########################################################################## THE END ######################################################################"],"execution_count":19,"outputs":[]}]}